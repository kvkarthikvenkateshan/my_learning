{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the computer science problems are solved by writing a series of instruction, but not at all the problems can be solved using this approach, for example a speech to text conversion system, there are millions to billions of words and it is a diffcult task to teach each adn every word and more over pronouncation,accent etc. differs, so for these type of problems the solution is to train the computer with an algorithm to understand some words so that it can learn by itself.This concept is called machine learning\n",
    "\n",
    "Key types of machine learning problem\n",
    "* Supervised : Learn to predict target values from labelled data\n",
    "    * Classification (target values are discrete classes)\n",
    "    * Regression (target values are continuous values)\n",
    "* Unsupervised : Find structure in unlabelled data\n",
    "    * Clustering : Find groups of similar instances in data\n",
    "    * Outlier Detection : Finding usual patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "fruits = pd.read_table(r'C:\\Users\\kkv1\\Desktop\\Python\\DS\\Applied ML in python\\Dataset\\fruit_data_with_colors.txt')\n",
    "\n",
    "feature_names_fruits = ['height', 'width', 'mass', 'color_score']\n",
    "X_fruits = fruits[feature_names_fruits]\n",
    "y_fruits = fruits['fruit_label']\n",
    "target_names_fruits = ['apple', 'mandarin', 'orange', 'lemon']\n",
    "\n",
    "X_fruits_2d = fruits[['height', 'width']]\n",
    "y_fruits_2d = fruits['fruit_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits, y_fruits, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# we must apply the scaling to the test set that we computed for the training set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test_scaled, y_test)))\n",
    "\n",
    "example_fruit = [[5.5, 2.2, 10, 0.70]]\n",
    "print('Predicted fruit type for ', example_fruit, ' is ', \n",
    "      target_names_fruits[knn.predict(example_fruit)[0]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from adspy_shared_utilities import load_crime_dataset\n",
    "\n",
    "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "\n",
    "\n",
    "# synthetic dataset for simple regression\n",
    "from sklearn.datasets import make_regression\n",
    "plt.figure()\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
    "                            n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# synthetic dataset for more complex regression\n",
    "from sklearn.datasets import make_friedman1\n",
    "plt.figure()\n",
    "plt.title('Complex regression problem with one input variable')\n",
    "X_F1, y_F1 = make_friedman1(n_samples = 100,\n",
    "                           n_features = 7, random_state=0)\n",
    "\n",
    "plt.scatter(X_F1[:, 2], y_F1, marker= 'o', s=50)\n",
    "plt.show()\n",
    "\n",
    "# synthetic dataset for classification (binary) \n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with two informative features')\n",
    "X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,\n",
    "                                n_redundant=0, n_informative=2,\n",
    "                                n_clusters_per_class=1, flip_y = 0.1,\n",
    "                                class_sep = 0.5, random_state=0)\n",
    "plt.scatter(X_C2[:, 0], X_C2[:, 1], c=y_C2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# more difficult synthetic dataset for classification (binary) \n",
    "# with classes that are not linearly separable\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2, centers = 8,\n",
    "                       cluster_std = 1.3, random_state = 4)\n",
    "y_D2 = y_D2 % 2\n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with non-linearly separable classes')\n",
    "plt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Breast cancer dataset for classification\n",
    "cancer = load_breast_cancer()\n",
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "\n",
    "\n",
    "# Communities and Crime dataset\n",
    "(X_crime, y_crime) = load_crime_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fruits = pd.read_table(r'C:\\Users\\kkv1\\Desktop\\Python\\DS\\Applied ML in python\\Dataset\\fruit_data_with_colors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fruits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fruits.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a mapping from fruit label value to fruit name to make results easier to interpret\n",
    "lookup_fruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique()))   \n",
    "lookup_fruit_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The file contains the mass, height, and width of a selection of oranges, lemons and apples. The heights were measured along the core of the fruit. The widths were the widest width perpendicular to the height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any machine learning task to train the computer we have split the data into two parts\n",
    "* Training set\n",
    "* Test set\n",
    "\n",
    "Training set is used to train the model and test set is used to evaluate the learned model.\n",
    "\n",
    "For creating a model all the features from the dataset might not be required so for that reason we take only those features which are revelant to the model we are creating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = fruits[['mass', 'width', 'height']]\n",
    "y = fruits['fruit_label']\n",
    "\n",
    "# default is 75% / 25% train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in machine learning is to evaluating the dataset, for this any visualization method can be used or one can simply scrol through the data.The reason for evaluating the data is as follows,\n",
    "* Type of cleaning or prep processing that is required\n",
    "* Distribution of values for each feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting a scatter matrix\n",
    "from matplotlib import cm\n",
    "\n",
    "X = fruits[['height', 'width', 'mass', 'color_score']]\n",
    "y = fruits['fruit_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "cmap = cm.get_cmap('gnuplot')\n",
    "scatter = pd.scatter_matrix(X_train, c= y_train, marker = 'o', s=40, hist_kwds={'bins':15}, figsize=(9,9), cmap=cmap)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting a 3D scatter plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c = y_train, marker = 'o', s=100)\n",
    "ax.set_xlabel('width')\n",
    "ax.set_ylabel('height')\n",
    "ax.set_zlabel('color_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "* k-NN classifiers are an example of what's called instance based or memory based supervised learning. What this means is that instance based learning methods work by memorizing the labeled examples that they see in the training set. And then they use those memorized examples to classify new objects later.\n",
    "* The k in k-NN refers to the number of nearest neighbors the classifier will retrieve and use in order to make its prediction. \n",
    "#### The k-Nearest Neighbor (k-NN) classifier algorithm\n",
    "* FInd the most similar instances to X_test that are in X_train\n",
    "* Get the labels of y_NN for the instances in X_NN\n",
    "* predict the label by combining the labels y_NN\n",
    "#### A nearest neighbor algorithm needs four things specified\n",
    "1. A distance metric\n",
    "2. How many nearest neighbors to look at?\n",
    "3. Optional weighting function on the neighbor points\n",
    "4. Method of aggregating the classes of neighbor points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classifier object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For this example, we use the mass, width, and height features of each fruit instance\n",
    "X = fruits[['mass', 'width', 'height']]\n",
    "y = fruits['fruit_label']\n",
    "\n",
    "# default is 75% / 25% train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the classifier (fit the estimator) using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate the accuracy of the classifier on future data, using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the trained k-NN classifier model to classify new, previously unseen objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first example: a small fruit with mass 20g, width 4.3 cm, height 5.5 cm\n",
    "fruit_prediction = knn.predict([[20, 4.3, 5.5]])\n",
    "lookup_fruit_name[fruit_prediction[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# second example: a larger, elongated fruit with mass 100g, width 6.3 cm, height 8.5 cm\n",
    "fruit_prediction = knn.predict([[100, 6.3, 8.5]])\n",
    "lookup_fruit_name[fruit_prediction[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the decision boundaries of the k-NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_fruit_knn\n",
    "\n",
    "plot_fruit_knn(X_train, y_train, 1, 'uniform')   # we choose 5 nearest neighbors\n",
    "plot_fruit_knn(X_train, y_train, 5, 'uniform') \n",
    "plot_fruit_knn(X_train, y_train, 10, 'uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How sensitive is k-NN classification accuracy to the choice of the 'k' parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_range = range(1,20)\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    scores.append(knn.score(X_test, y_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, scores)\n",
    "plt.xticks([0,5,10,15,20]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How sensitive is k-NN classification accuracy to the train/test split proportion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for s in t:\n",
    "\n",
    "    scores = []\n",
    "    for i in range(1,1000):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s)\n",
    "        knn.fit(X_train, y_train)\n",
    "        scores.append(knn.score(X_test, y_test))\n",
    "    plt.plot(s, np.mean(scores), 'bo')\n",
    "\n",
    "plt.xlabel('Training set proportion (%)')\n",
    "plt.ylabel('accuracy');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "fruits = pd.read_table(r'C:\\Users\\kkv1\\Desktop\\Python\\DS\\Applied ML in python\\Dataset\\fruit_data_with_colors.txt')\n",
    "\n",
    "X = fruits[['height','width','mass','color_score']]\n",
    "y = fruits['fruit_label']\n",
    "\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train,y_train)\n",
    "print('Accuracy of knn classifier on the test set:', knn.score(X_test,y_test))\n",
    "\n",
    "example_fruit = [[5.5,2.2,10,0.70]]\n",
    "print('predicted fruit type for ', example_fruit, 'is ', knn.predict(example_fruit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Supervised learnign can be divided into two \n",
    "* Classification\n",
    "* Regression\n",
    "\n",
    "Both classification and regression take a set of training instances and learn a mapping to a target value.\n",
    "* For classification, the target value is a discrete class value\n",
    "        Ex: Binary: Deciding whether a transaction is fradulant or not\n",
    "            Multiclass : Target value is one of a set of discrete values (the fruit dataset is a multi class)\n",
    "            Multilabel : Example classifying pages into multiple topics\n",
    "            \n",
    "* For regression, the target value is continous (real-values/floating point)\n",
    "        Ex: Predicting the selling prize of a house from it's attributes\n",
    "    * Looking at the target value's type will guide you on what supervised learning method is to be use.\n",
    "    * Many supervised learning methods have flavors for classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "Overfitting typically occurs when we try to fit a complex model with an inadequate amount of training data. An overfitted model uses its ability to capture complex patterns by being great at predicting lots and lots of specific data samples or areas of local variation in the training set. But it often misses seeing global patterns in the training set that would help it generalize well on the unseen test set. \n",
    "\n",
    "In general for a classifier as value of k decreases the risk of overfitting increases, this is because as k decresase say k = 1, now the classifier is affected by noise and outliers and there by decision boundary changes.\n",
    "\n",
    "#### Underfitting\n",
    "In case of under fitted model the learned model may not be even able to predict/classify the test vlaues.\n",
    "\n",
    "#### Overfitting and Underfitting for Regression\n",
    "In case of linear regresssion with an underfitted model the RSS (residual sum of saqures) is high and there by the predictions from the model will not be accurate, on the other hand in case of overfitted model that linear regression curve is of higher order with the intension of decresing the RSS but with this approach it tries to cover all the aspects but will not be able to generalize global pattern.\n",
    "    \n",
    "#### Overfitting and Underfitting for Classification\n",
    "In case of knn classifier with an underfitted model the classification doesn't happen properly coz the model has not considered the majority of the points for classification but, on the other hand in case of overfitted model that knn boundary tries to classify considering all the points , but while doing so it may leave out the obvious global pattern i.e. in order to put a outlier point inside a particular group boundary the model would go on to extend the boundary which leads to undesired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features = 1, n_informative = 1,bias = 150.0, noise = 30, random_state = 0)\n",
    "\n",
    "plt.scatter(X_R1, y_R1, marker = 'o', s= 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_R1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* n_samples = 100 is the number of samples you want to generate\n",
    "* n_features = 1 is the number of columns you want for your variable in the above case X_R1, as of now it has 1 column if 2 then 2 columns will be created\n",
    "* n_informative = 1, The number of informative features, i.e., the number of features used to build the linear model used to generate the output.\n",
    "* bias = 150.0, The bias term in the underlying linear model. [i think it is the difference between X_R1 and y_R1]\n",
    "* noise = 30 The standard deviation of the gaussian noise applied to the output.\n",
    "* random_state = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Sample Classification problem with two informative features')\n",
    "X_C2, y_C2 = make_classification(n_samples = 100, n_features = 2, \n",
    "                                 n_informative = 2, n_redundant = 0, \n",
    "                                 n_clusters_per_class = 1 , flip_y = 0.1, \n",
    "                                 class_sep = 0.5, random_state = 0)\n",
    "\n",
    "plt.scatter(X_C2[:, 0], X_C2[:, 1], c=y_C2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_C2#[: , 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(X_C2),len(y_C2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_two_class_knn\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n",
    "                                                   random_state=0)\n",
    "\n",
    "plot_two_class_knn(X_train, y_train, 1, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 3, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 11, 'uniform', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of figure 3 where k=1 the classifier is doing overfitting by considering each and every point and it can also be observed that for k=1 the train score = 1 and test score = 0.8.\n",
    "\n",
    "Now as k incereases to 3 i.e. k=3, training score decreases and test score decreases marginally.\n",
    "i.e. train score = 0.92 and test score = 0.72\n",
    "\n",
    "Again for k=11 training score has further decreased to 0.77 while the test score increases to 0.8, the other important thing to observe here is the more generalized patter that has come in terms of boundary. which menas the model is able to see the more generalized pattern.\n",
    "\n",
    "\n",
    "#### Knn can also be used to for regression models.\n",
    "In this case the depending on the k value the model will consider those many nearest points to the query point and then take the average of all these points to predict the target value,In case of a knn regresser also as the value of k decreases the risk of overfitting increases and as the value of increases the model tries to understand the more global pattern.\n",
    "\n",
    "#### R-Squared regression score [Co-efiicient of determination]\n",
    "Measure how well a prediction model for regression fits the given data. The score is between 0 to 1.\n",
    "* A value of 0 corresponds to a constant model that predicts the mean value of all training target values.\n",
    "* A value of 1 corresponds to prefect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state = 0)\n",
    "\n",
    "knnreg = KNeighborsRegressor(n_neighbors = 5).fit(X_train, y_train)\n",
    "\n",
    "print(knnreg.predict(X_test))\n",
    "print('R-squared test score: {:.3f}'\n",
    "     .format(knnreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, subaxes = plt.subplots(1, 2, figsize=(8,4))\n",
    "X_predict_input = np.linspace(-3, 3, 50).reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1[0::5], y_R1[0::5], random_state = 0)\n",
    "\n",
    "for thisaxis, K in zip(subaxes, [1, 3]):\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = K).fit(X_train, y_train)\n",
    "    y_predict_output = knnreg.predict(X_predict_input)\n",
    "    thisaxis.set_xlim([-2.5, 0.75])\n",
    "    thisaxis.plot(X_predict_input, y_predict_output, '^', markersize = 10,\n",
    "                 label='Predicted', alpha=0.8)\n",
    "    thisaxis.plot(X_train, y_train, 'o', label='True Value', alpha=0.8)\n",
    "    thisaxis.set_xlabel('Input feature')\n",
    "    thisaxis.set_ylabel('Target value')\n",
    "    thisaxis.set_title('KNN regression (K={})'.format(K))\n",
    "    thisaxis.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot k-NN regression on sample dataset for different values of K\n",
    "fig, subaxes = plt.subplots(5, 1, figsize=(5,20))\n",
    "X_predict_input = np.linspace(-3, 3, 500).reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "for thisaxis, K in zip(subaxes, [1, 3, 7, 15, 55]):\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = K).fit(X_train, y_train)\n",
    "    y_predict_output = knnreg.predict(X_predict_input)\n",
    "    train_score = knnreg.score(X_train, y_train)\n",
    "    test_score = knnreg.score(X_test, y_test)\n",
    "    thisaxis.plot(X_predict_input, y_predict_output)\n",
    "    thisaxis.plot(X_train, y_train, 'o', alpha=0.9, label='Train')\n",
    "    thisaxis.plot(X_test, y_test, '^', alpha=0.9, label='Test')\n",
    "    thisaxis.set_xlabel('Input feature')\n",
    "    thisaxis.set_ylabel('Target value')\n",
    "    thisaxis.set_title('KNN Regression (K={})\\n\\\n",
    "Train $R^2 = {:.3f}$,  Test $R^2 = {:.3f}$'\n",
    "                      .format(K, train_score, test_score))\n",
    "    thisaxis.legend()\n",
    "    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the value of k increases (till optimum) the Test R^2  increases and train R^2 decreases as shown in above 4 figure with values of k 1, 7, 15, 55. Till k=15 the preformance of the model was increasing but at k=55 the model is underfitting i.e it is not able to work even on the training data.\n",
    "\n",
    "Advantages of Knn are:\n",
    "* Simple and easy approach\n",
    "* knn approch can be reasonable baseline for comparison against more sophisticated methods\n",
    "\n",
    "Cons for knnn are\n",
    "* If the number of feature are more the performance of the k decreases\n",
    "* In the dataset that has sparse data (many columns are there but with majority of the data as 0)\n",
    "\n",
    "K-nearest neighbors and linear model fitting using least squares are two complementary approaches to supervised learning. K-nearest neighbors doesn't make a lot of assumptions about the structure of the data and gives potentially accurate but sometimes unstable predictions here unstable means that it could be sensitive to small changes in the training data. \n",
    "\n",
    "On the other hand, linear models make strong assumptions about the structure of the data. In other words, that the target value can be predicted just using a weighted sum of the input variables, a linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://pasteboard.co/3fS7MBaO.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"accuracy_vs_complexity.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from the above figure initially the model accuracy increase with increase in model complexity but after certain point it stats decreasing and coz with increase in complexity[no of features] and with same value or lesser value of k the model tries to overfit resulting in reduction of the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a model?\n",
    "\n",
    "It's a specific mathematical or computational description that expresses the relationship between a set of input variables and one or more outcome variables that are being studied or predicted. \n",
    "\n",
    "In statistical terms, the input variables are called independent variables. And the outcome variables are termed dependent variables. \n",
    "\n",
    "In machine learning, we use the term features to refer to the input or independent variables, and target value or target label to refer to the output dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models \n",
    "\n",
    "A linear model is a sum of weighted variables that predicts a target output value given an input data instance.\n",
    "\n",
    "Linear models make a strong prior assumption about the relationship between the input x and output y. Linear models may seem simplistic. But for data with many features, linear models can be very effective and generalize well to new data beyond the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Linear_regression.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50, alpha=0.8)\n",
    "plt.plot(X_R1, linreg.coef_ * X_R1 + linreg.intercept_, 'r-')\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ridge regression learns w, busing the same least-squares criterion but adds a penalty for large variations in wparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Ridge_regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once the parameters are learned, the ridge regression prediction formula is the sameas ordinary least-squares.\n",
    "* The addition of a parameter penalty is called regularization. Regularization prevents overfitting by restricting the model, typically to reduce its complexity.\n",
    "* Ridge regression uses L2 regularization: minimize sum of squares of w-entries\n",
    "* The influence of the regularization term is controlled by the ùõº parameter.\n",
    "* Higher alpha means more regularization and simpler models\n",
    "* The regularization term prevents overfitting becasue it is adding to the RHS i.e. to sum of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import load_crime_dataset\n",
    "(X_crime, y_crime) = load_crime_dataset()\n",
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need for Feature Normalization\n",
    "\n",
    "* Important for some machine learning methods that all features are on the same scale (e.g. faster convergence in learning, more uniform or 'fair' influence for all weights)\n",
    "        e.g. regularized regression, k-NN, support vector machines, neural networks\n",
    "* One method of establishing the feature normalization is to use minmax scaler function, this method normalises all the feature to lie between 0 and 1, with 1 for max value in column and 0 for min value. With this approach all the features have equal weightage.\n",
    "* For each feature ùë•ùëñ: compute the min value ùë•ùëñùëÄin and the max value ùë•iùëÄax achieved across all instances in the training set.\n",
    "* For each feature: transform a given feature ùë•ùëñ value to a scaled version ùë•ùëñ‚Ä≤using the formula\n",
    "        xi' = (xi - xmin)/(ximax - ximin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled= scaler.transform(X_train)\n",
    "\n",
    "In the above code fit and transform are used differently, however it can be used as one as shown below\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled= scaler.fit_transform(X_train)\n",
    "\n",
    "* The test set must use identical scaling to the training set but without the use of fit.\n",
    "    fit_transform should be used on training set data and only transform should be used on test set to avoid data leakage.\n",
    "    \n",
    "#### Disadvantage of doing feature Normalization.\n",
    "\n",
    "The resulting model and the transformed features may be harder to interpret. \n",
    "\n",
    "\n",
    "In general, regularisation works especially well when you have relatively small amounts of training data compared to the number of features in your model. Regularisation becomes less important as the amount of training data you have increases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with regularization parameter: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above values of alpha it can be seen that initially the r-squared test value increases with increase in alpha but later it starts to decrease with further increase in alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "    \n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression is another form of regularized linear regression that uses an L1 regularization penalty for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Lasso_regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This has the effect of setting parameter weights to zero for the least influential variables. This is called a sparsesolution: a kind of feature selection\n",
    "* The parameter ùõº controls amount of L1 regularization (default = 1.0).\n",
    "* The prediction formula is the same as ordinary least-squares.\n",
    "* When to use ridge vs lasso regression:\n",
    "        ‚Äì Many small/medium sized effects: use ridge.\n",
    "        ‚Äì Only a few variables with medium/large effect: use lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=2.0, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X_crime), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso regression with regularization parameter: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output it can also be oberseved that as alpha increases the number of features with non zero weightage decreases, i.e with increase in alpha the more and more coefficient are set to zero in case of a Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.datasets import make_friedman1\n",
    "\n",
    "X_F1, y_F1 = make_friedman1(n_samples = 100,\n",
    "                           n_features = 7, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nNow we transform the original input data to add\\n\\\n",
    "polynomial features up to degree 2 (quadratic)\\n')\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_F1_poly = poly.fit_transform(X_F1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2) R-squared score (test): {:.3f}\\n'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nAddition of many polynomial features often leads to\\n\\\n",
    "overfitting, so we often use polynomial features in combination\\n\\\n",
    "with regression that has a regularization penalty, like ridge\\n\\\n",
    "regression.\\n')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2 + ridge) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2 + ridge) R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Linera_regression_model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Logistic_regression.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from adspy_shared_utilities import (\n",
    "plot_class_regions_for_classifier_subplot)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "y_fruits_apple = y_fruits_2d == 1   # make into a binary problem: apples vs everything else\n",
    "X_train, X_test, y_train, y_test = (\n",
    "train_test_split(X_fruits_2d.as_matrix(),\n",
    "                y_fruits_apple.as_matrix(),\n",
    "                random_state = 0))\n",
    "\n",
    "clf = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None,\n",
    "                                         None, 'Logistic regression \\\n",
    "for binary classification\\nFruit dataset: Apple vs others',\n",
    "                                         subaxes)\n",
    "\n",
    "h = 6\n",
    "w = 8\n",
    "print([clf.predict([[h,w]])[0]])\n",
    "print('A fruit with height {} and width {} is predicted to be: {}'\n",
    "     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n",
    "\n",
    "h = 10\n",
    "w = 7\n",
    "print([clf.predict([[h,w]])[0]])\n",
    "print('A fruit with height {} and width {} is predicted to be: {}'\n",
    "     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n",
    "subaxes.set_xlabel('height')\n",
    "subaxes.set_ylabel('width')\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from adspy_shared_utilities import (\n",
    "plot_class_regions_for_classifier_subplot)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "title = 'Logistic regression, simple synthetic dataset C = {:.3f}'.format(1.0)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                         None, None, title, subaxes)\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = (\n",
    "train_test_split(X_fruits_2d.as_matrix(),\n",
    "                y_fruits_apple.as_matrix(),\n",
    "                random_state=0))\n",
    "\n",
    "fig, subaxes = plt.subplots(3, 1, figsize=(4, 10))\n",
    "\n",
    "for this_C, subplot in zip([0.1, 1, 100], subaxes):\n",
    "    clf = LogisticRegression(C=this_C).fit(X_train, y_train)\n",
    "    title ='Logistic regression (apple vs rest), C = {:.3f}'.format(this_C)\n",
    "    \n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                             X_test, y_test, title,\n",
    "                                             subplot)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "print('Breast cancer dataset')\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifiers\n",
    "\n",
    "\n",
    "A linear classifier is a function that maps an input data point x to an output class value y(+1 or -1) using a linear function with weight parameters w of the input point's features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Linear_classifier.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above equation wi is weightage of the cofficeient and xi is the value for the points. Dot product of this is computed as show in the firgure\n",
    "\n",
    "If w1 and w2 are coefficients and x1 and x2 are the points and if b=0, then,\n",
    "\n",
    "Dot product\n",
    "\n",
    "(w1,w2)o(x1,x2)\n",
    "\n",
    "(w1 x1) + (w2 x2)\n",
    "\n",
    "If the above case the line considered is x1- x2 = 0 and there by w1 and w2 are 1 and -1 respectively\n",
    "so on considering any point in the plain we just need to substitute the value of x1 and x2 to find out whether resultant value is greater than or less than zero.\n",
    "\n",
    "If the value is greater than 0 then it is considered as belonging to class +1 else to class -1.This classification is done by the sign in the above formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Margin\n",
    "\n",
    "Defined as the maximum width the decision boundary area can be increased before hitting a data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"classifier_margin.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Support Vector Machines\n",
    "#### Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "this_C = 1.0\n",
    "clf = SVC(kernel = 'linear', C=this_C).fit(X_train, y_train)\n",
    "title = 'Linear SVC, C = {:.3f}'.format(this_C)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None, None, title, subaxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Support Vector Machine: C parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\n",
    "fig, subaxes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for this_C, subplot in zip([0.00001, 100], subaxes):\n",
    "    clf = LinearSVC(C=this_C).fit(X_train, y_train)\n",
    "    title = 'Linear SVC, C = {:.5f}'.format(this_C)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                             None, None, title, subplot)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The strength of regularization is determined by C\n",
    "* Larger values of C: less regularization\n",
    "        ‚ÄìFit the training data as well as possible\n",
    "        ‚ÄìEach individual data point is important to classify correctly\n",
    "* Smaller values of C: more regularization\n",
    "        ‚ÄìMore tolerant of errors on individual data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Multiclass_classification.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* scikit-learn automatically detects a multiclass classification problem.\n",
    "* scikit-learn makes it very easy to learn multiclass classification models by converting a multiclass classification problem into a series of binary problems.\n",
    "*  Scikit-learn creates one binary classifier that predicts that class against all the other classes.\n",
    "            example: In the fruit dataset there are four categories of fruit. So scikit-learn learns four different binary classifiers.\n",
    "* To predict a new data instance, it takes data instance whose labels to be predict, and runs it against each of the binary classifiers in turn, and the classifier that has the highest score is the category to which this instance of data belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
